{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function pretty_print_review_and_label(i)\n",
    "    println(labels[i] * \"\\t\\t\" * reviews[i][:80]*\"...\")\n",
    "end\n",
    "g = open(\"reviews.txt\", \"r\");\n",
    "reviews = map(x -> x[1:end-1], readlines(g))\n",
    "close(g)\n",
    "\n",
    "g = open(\"labels.txt\", \"r\")\n",
    "labels = map(x -> x[1:end-1], readlines(g))\n",
    "close(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capturing Word Correlation in Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent Encoding:[1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "onehots = Dict()\n",
    "onehots[\"cat\"] = [1,0,0,0]\n",
    "onehots[\"the\"] = [0,1,0,0]\n",
    "onehots[\"dog\"] = [0,0,1,0]\n",
    "onehots[\"sat\"] = [0,0,0,1]\n",
    "\n",
    "sentence = [\"the\",\"cat\",\"sat\"]\n",
    "x = onehots[sentence[1]] +\n",
    "    onehots[sentence[2]] +\n",
    "    onehots[sentence[3]]\n",
    "\n",
    "println(\"Sent Encoding:\" ,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"reviews.txt\")\n",
    "raw_reviews = readlines(f)\n",
    "close(f)\n",
    "\n",
    "f = open(\"labels.txt\")\n",
    "raw_labels = readlines(f)\n",
    "close(f)\n",
    "\n",
    "tokens = collect(Set(map(x -> split(x, \" \"), raw_reviews)))\n",
    "\n",
    "vocab = Set()\n",
    "for sent in tokens\n",
    "    for word in sent\n",
    "        if length(word)>0\n",
    "            push!(vocab, word)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "vocab = collect(vocab)\n",
    "\n",
    "word2index = Dict()\n",
    "for (i,word) in enumerate(vocab)\n",
    "    word2index[word] = i\n",
    "end\n",
    "\n",
    "input_dataset = []\n",
    "for sent in tokens\n",
    "    sent_indices = []\n",
    "    for word in sent\n",
    "        try\n",
    "            push!(sent_indices, word2index[word])\n",
    "        catch\n",
    "            nothing\n",
    "        end\n",
    "    end\n",
    "    push!(input_dataset, sent_indices)\n",
    "end\n",
    "\n",
    "target_dataset = []\n",
    "for label in raw_labels\n",
    "    if label == \"positive\"\n",
    "        push!(target_dataset, 1)\n",
    "    else\n",
    "        push!(target_dataset, 0)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1 Progress: 95.97% Training Accuracy: 0.447801163228587%   \n",
      "Iter: 2 Progress: 95.97% Training Accuracy: 0.4768310286395682%  \n",
      "Test Accuracy: 0.504\n"
     ]
    }
   ],
   "source": [
    "using Random: seed!\n",
    "seed!(1);\n",
    "\n",
    "sigmoid(x) = 1/(1 + exp(-x))\n",
    "\n",
    "alpha, iterations = (0.01, 2)\n",
    "hidden_size = 100\n",
    "\n",
    "weights_0_1 = 0.2 .* rand(hidden_size, length(vocab)) .- 0.1\n",
    "weights_1_2 = 0.2 .* rand(1, hidden_size) .- 0.1\n",
    "\n",
    "correct,total = (0,0)\n",
    "\n",
    "for iter=1:iterations\n",
    "    global correct,total\n",
    "    \n",
    "    for i=1:length(input_dataset)-1000\n",
    "        x,y = (input_dataset[i],target_dataset[i])\n",
    "        layer_1 = sigmoid.(sum(weights_0_1[:,x]; dims=2)) #embed + sigmoid\n",
    "        layer_2 = sigmoid.(weights_1_2 * layer_1) # linear + softmax\n",
    "\n",
    "        layer_2_delta = layer_2[1] - y # compare pred with truth\n",
    "        layer_1_delta = weights_1_2' * layer_2_delta #backprop\n",
    "        layer_2_delta .* layer_1\n",
    "        weights_0_1[:,x] .-= layer_1_delta .* alpha\n",
    "        weights_1_2 .-= layer_2_delta .* layer_1' .* alpha\n",
    "        \n",
    "        if abs(layer_2_delta) < 0.5\n",
    "            correct += 1\n",
    "        end\n",
    "        total += 1\n",
    "        \n",
    "        if (i%10 == 9)\n",
    "            progress = string(i/length(input_dataset))\n",
    "            print(\"Iter: $(iter) Progress: $(progress[3:4]).$(progress[5:6])% Training Accuracy: $(correct/total)% \\r\")\n",
    "        end\n",
    "    end\n",
    "    println()\n",
    "end\n",
    "\n",
    "correct,total = (0,0)\n",
    "for i=length(input_dataset)-1000+1:length(input_dataset)\n",
    "    global correct,total\n",
    "    x = input_dataset[i]\n",
    "    y = target_dataset[i]\n",
    "    \n",
    "    layer_1 = sigmoid.(sum(weights_0_1[:,x]; dims=2))\n",
    "    layer_2 = sigmoid.(weights_1_2 * layer_1)\n",
    "    \n",
    "    if abs(layer_2[1] - y) < 0.5\n",
    "        correct += 1\n",
    "    end\n",
    "    total += 1 \n",
    "end\n",
    "\n",
    "println(\"Test Accuracy: $(correct / total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168-element Array{SubString{String},1}:\n",
       " \"ok\"\n",
       " \"now\"\n",
       " \"\"\n",
       " \"lets\"\n",
       " \"see\"\n",
       " \".\"\n",
       " \"what\"\n",
       " \"was\"\n",
       " \"funny\"\n",
       " \"in\"\n",
       " \"the\"\n",
       " \"first\"\n",
       " \"movie\"\n",
       " â‹®\n",
       " \"out\"\n",
       " \"of\"\n",
       " \"ten\"\n",
       " \"since\"\n",
       " \"it\"\n",
       " \"has\"\n",
       " \"some\"\n",
       " \"funny\"\n",
       " \"parts\"\n",
       " \".\"\n",
       " \"\"\n",
       " \"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "similar (generic function with 2 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function similar(target = \"beautiful\")\n",
    "    target_index = word2index[target]\n",
    "    scores = Dict()\n",
    "    for (word,index) in word2index\n",
    "        raw_difference = weights_0_1[:,index] .- (weights_0_1[:,target_index])\n",
    "        squared_difference = raw_difference .* raw_difference\n",
    "        scores[word] = -sqrt(sum(squared_difference))\n",
    "    end\n",
    "    scores = sort(collect(scores), by = x -> x[2])\n",
    "    return scores[end-10:end]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair{Any,Any}[\"filmy\" => -0.6606711188723169, \"confab\" => -0.6600965247133369, \"cmara\" => -0.6600060084860495, \"parameters\" => -0.6591945678386412, \"theron\" => -0.6576485630760832, \"faithful\" => -0.6561845107874287, \"deluders\" => -0.6555951081367472, \"diwana\" => -0.6501706963541887, \"alliances\" => -0.6497885466271454, \"brasseur\" => -0.6434690598261698, \"beautiful\" => -0.0]"
     ]
    }
   ],
   "source": [
    "print(similar(\"beautiful\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair{Any,Any}[\"fangs\" => -0.666296956359614, \"sleazier\" => -0.6646779152677983, \"regrettable\" => -0.6624437193693834, \"abortion\" => -0.6613846456145038, \"ghoulies\" => -0.6611451961709435, \"inveighing\" => -0.6547246882828751, \"kkk\" => -0.6545768734779763, \"sumo\" => -0.6449526409110887, \"disinherit\" => -0.6203307006148524, \"benedick\" => -0.6051667690036869, \"terrible\" => -0.0]"
     ]
    }
   ],
   "source": [
    "print(similar(\"terrible\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Random: seed!, shuffle!\n",
    "using Statistics: mean\n",
    "seed!(1)\n",
    "\n",
    "f = open(\"reviews.txt\")\n",
    "raw_reviews = readlines(f)\n",
    "close(f)\n",
    "\n",
    "tokens = collect(Set(map(x -> split(x, \" \"), raw_reviews)))\n",
    "\n",
    "vocab = Set()\n",
    "for sent in tokens\n",
    "    for word in sent\n",
    "        push!(vocab, word)\n",
    "    end\n",
    "end\n",
    "vocab = collect(vocab)\n",
    "pushfirst!(vocab, \"\")\n",
    "\n",
    "word2index = Dict()\n",
    "for (i,word) in enumerate(vocab)\n",
    "    word2index[word] = i\n",
    "end\n",
    "\n",
    "\n",
    "concatenated = []\n",
    "input_dataset = []\n",
    "\n",
    "for sent in tokens\n",
    "    sent_indices = []\n",
    "    for word in sent\n",
    "        try\n",
    "            push!(sent_indices, word2index[word])\n",
    "            push!(concatenated, word2index[word])\n",
    "        catch\n",
    "            nothing\n",
    "        end\n",
    "    end\n",
    "    push!(input_dataset, sent_indices)\n",
    "end\n",
    "shuffle!(input_dataset);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 49751 Progress: 99.57% Pair{Any,Any}[\"horrid\" => -4.968490509679609, \"horrendous\" => -4.9469975512341176, \"superb\" => -4.9356506147520545, \"lame\" => -4.8300165682816605, \"magnificent\" => -4.761170873823829, \"fantastic\" => -4.458455070159601, \"brilliant\" => -4.447408842287923, \"dreadful\" => -4.43556141765357, \"pathetic\" => -4.434110473622522, \"horrible\" => -3.343947102556384, \"terrible\" => -0.0] ]   .0] 0]  \n",
      "Pair{Any,Any}[\"horrid\" => -4.982131116812594, \"horrendous\" => -4.9469975512341176, \"superb\" => -4.9356506147520545, \"lame\" => -4.803823641923124, \"magnificent\" => -4.761170873823829, \"pathetic\" => -4.5436237709839995, \"brilliant\" => -4.462830518799266, \"fantastic\" => -4.462683434294019, \"dreadful\" => -4.43556141765357, \"horrible\" => -3.343947102556384, \"terrible\" => -0.0]"
     ]
    }
   ],
   "source": [
    "alpha, iterations = (0.05, 2)\n",
    "hidden_size,window,negative = (50,2,5)\n",
    "\n",
    "weights_0_1 = (rand(hidden_size, length(vocab)) .- 0.5) .* 0.2\n",
    "weights_1_2 = zeros(hidden_size, length(vocab))\n",
    "\n",
    "layer_2_target = zeros(negative+1)\n",
    "layer_2_target[1] = 1\n",
    "\n",
    "function similar(target = \"beautiful\")\n",
    "    target_index = word2index[target]\n",
    "    scores = Dict()\n",
    "    for (word,index) in word2index\n",
    "        raw_difference = weights_0_1[:,index] .- (weights_0_1[:,target_index])\n",
    "        squared_difference = raw_difference .* raw_difference\n",
    "        scores[word] = -sqrt(sum(squared_difference))\n",
    "    end\n",
    "    scores = sort(collect(scores), by = x -> x[2])\n",
    "    return scores[end-10:end]\n",
    "end\n",
    "\n",
    "sigmoid(x) = 1/(1 + exp(-x))\n",
    "\n",
    "for (rev_i,review) in enumerate(repeat(input_dataset, iterations))\n",
    "    for target_i=1:length(review)\n",
    "    # since it's really expensive to predict every vocabulary\n",
    "    # we're only going to predict a random subset  \n",
    "        target_samples = cat([review[target_i]],\n",
    "            concatenated[floor.(Int, rand(negative) .* length(concatenated)) .+ 1];dims=1)\n",
    "        \n",
    "        left_context = review[maximum([1,target_i-window]):target_i-1]\n",
    "        right_context = review[target_i+1:minimum([length(review),target_i+window])]\n",
    "        \n",
    "        layer_1 = mean(weights_0_1[:,cat(left_context,right_context;dims=1)];dims=2)\n",
    "        layer_2 = sigmoid.(weights_1_2[:,target_samples]' * layer_1)\n",
    "        \n",
    "        layer_2_delta = layer_2 .- layer_2_target\n",
    "        layer_1_delta = weights_1_2[:,target_samples] * layer_2_delta\n",
    "        \n",
    "        weights_0_1[:,cat(left_context,right_context;dims=1)] .-= layer_1_delta .* alpha\n",
    "        weights_1_2[:,target_samples] .-= layer_2_delta' .* layer_1 .* alpha\n",
    "    end\n",
    "    if ((rev_i-1)%250 ==0)\n",
    "        progress = string(rev_i/(length(input_dataset)*iterations))\n",
    "        print(\"Iter: $(rev_i) Progress: $(progress[3:4]).$(progress[5:6])% $(similar(\"terrible\")) \\r\")\n",
    "    end\n",
    "end\n",
    "println()\n",
    "print(similar(\"terrible\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King - Man + Woman ~= Queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "analogy (generic function with 3 methods)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function analogy(positive=[\"terrible\",\"good\"],negative=[\"bad\"])\n",
    "    norms = sum(weights_0_1 .* weights_0_1;dims=1)\n",
    "    normed_weights = weights_0_1 .* norms\n",
    "    \n",
    "    query_vect = zeros(length(weights_0_1[:,1]))\n",
    "    for word in positive\n",
    "        query_vect .+= normed_weights[:,word2index[word]]\n",
    "    end\n",
    "    for word in negative\n",
    "        query_vect .-= normed_weights[:,word2index[word]]\n",
    "    end\n",
    "    \n",
    "    scores = Dict()\n",
    "    for (word,index) in word2index\n",
    "        raw_difference = weights_0_1[:,index] .- query_vect\n",
    "        squared_difference = raw_difference .* raw_difference\n",
    "        scores[word] = -sqrt(sum(squared_difference))\n",
    "    end\n",
    "    scores = sort(collect(scores), by = x -> x[2])\n",
    "    return scores[end-10:end]\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Array{Pair{Any,Any},1}:\n",
       "   \"perfect\" => -405.2968233888864\n",
       "  \"terrific\" => -405.28219077252123\n",
       " \"wonderful\" => -405.2342512528658\n",
       "      \"nice\" => -405.209254088029\n",
       " \"fantastic\" => -405.0621007960378\n",
       "    \"superb\" => -404.7278584523943\n",
       "    \"decent\" => -404.715161833891\n",
       "     \"great\" => -404.5304307083761\n",
       "      \"fine\" => -404.42569607886657\n",
       "  \"terrible\" => -404.3960883623826\n",
       "      \"good\" => -404.0519207337501"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy([\"terrible\",\"good\"],[\"bad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Array{Pair{Any,Any},1}:\n",
       "      \"torn\" => -241.60412212995882\n",
       "    \"called\" => -241.57320833812165\n",
       "   \"wanders\" => -241.5723024402283\n",
       "        \"la\" => -241.49207813653848\n",
       "       \"ted\" => -241.49052615156592\n",
       "       \"san\" => -241.45081930042366\n",
       "    \"fallen\" => -241.3633913505415\n",
       "     \"henry\" => -241.26504120020587\n",
       " \"elizabeth\" => -241.1699390705535\n",
       "      \"anti\" => -241.12247331208425\n",
       "     \"hills\" => -240.4933871202859"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy([\"elizabeth\",\"he\"],[\"she\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
